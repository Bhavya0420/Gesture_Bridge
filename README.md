# ğŸ–ï¸ğŸ™ï¸ Gesture Bridge

### A Bi-Directional Sign Language Communication System

Gesture Bridge is an AI-powered application that enables **two-way communication** between **hearing-impaired** and **hearing users** by converting:

- âœ‹ **Sign Language â†’ Text**
- ğŸ™ï¸ **Voice â†’ Sign Language (GIFs & Letter Visuals)**

The system bridges the communication gap using **Computer Vision**, **Speech Recognition**, and **Deep Learning**.

---

## ğŸ“Œ Project Overview

Gesture Bridge integrates **real-time hand gesture recognition** with **voice-to-sign visualization** to create an inclusive communication platform.

- Uses a camera to detect hand landmarks and recognize gestures.
- Converts recognized gestures into readable text.
- Converts spoken words into **sign language GIFs** or **letter-by-letter visual actions**.
- Designed with a **simple GUI** for easy interaction.

---

## ğŸ¯ Key Features

### âœ‹ Sign to Text (Visual â†’ Text)

- Real-time hand gesture detection using **MediaPipe**.
- ASL/ISL gesture classification using a **Deep Learning model**.
- Displays detected gesture labels on-screen.
- Supports dataset creation and model retraining.

### ğŸ™ï¸ Voice to Sign (Speech â†’ Visual)

- Converts live voice input into text using **Speech Recognition**.
- If detected text matches predefined dictionary phrases:

  - Displays the **corresponding Sign Language GIF**.

- If not:

  - Breaks the word into letters.
  - Displays **letter-by-letter sign images** sequentially.

- Automatically exits when the user says **â€œgoodbyeâ€**.

### ğŸ–¥ï¸ User Interface

- Simple and interactive GUI using **EasyGUI**.
- Central menu with two modes:

  - Sign to Text
  - Voice to Sign

---

## ğŸ§­ Application Flow

### Main Menu

```
Gesture Bridge
â”‚
â”œâ”€â”€ Sign to Text
â”‚   â””â”€â”€ Camera â†’ Hand Detection â†’ Gesture Classification â†’ Text Output
â”‚
â”œâ”€â”€ Voice to Sign
â”‚   â””â”€â”€ Speech â†’ Text
â”‚       â”œâ”€â”€ If phrase in dictionary â†’ Display GIF
â”‚       â”œâ”€â”€ Else â†’ Display letter visuals sequentially
â”‚       â””â”€â”€ If â€œgoodbyeâ€ â†’ Exit
â”‚
â””â”€â”€ Exit
```

---

## ğŸ› ï¸ Technologies Used

- **Python 3.10**
- **OpenCV** â€“ Camera handling & visualization
- **MediaPipe** â€“ Hand landmark detection
- **TensorFlow** â€“ Gesture classification model
- **SpeechRecognition** â€“ Voice input processing
- **EasyGUI** â€“ GUI menu and dialogs
- **Pillow** â€“ Image & GIF handling
- **NumPy, Pandas, Scikit-learn**
- **Matplotlib, Seaborn** â€“ Visualization & debugging

---

## ğŸ“¦ Requirements

Create a virtual environment and install dependencies using:

```txt
numpy==1.26.4
opencv-contrib-python==4.8.1.78
mediapipe==0.10.9
tensorflow-intel==2.16.1
protobuf==3.20.3

Pillow
pandas
scikit-learn
matplotlib
seaborn
easygui
SpeechRecognition
```

> âš ï¸ **Note:** > `tkinter` comes pre-installed with Python on Windows and should NOT be added to `requirements.txt`.

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Bhavya0420/Gesture_Bridge.git
cd Gesture_Bridge
```

### 2ï¸âƒ£ Create & Activate Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run the Application

```bash
python app.py
```

---

## ğŸ® Usage Instructions

### â–¶ï¸ Sign to Text

- Select **Sign to Text** from the main menu.
- Show hand gestures in front of the camera.
- Detected gestures will be classified and displayed as text.
- Press **ESC** to exit.

### â–¶ï¸ Voice to Sign

- Select **Voice to Sign**.
- Choose **Live Voice**.
- Speak clearly into the microphone.
- Output:

  - Phrase GIF (if predefined).
  - Letter-by-letter sign images (if not).

- Say **â€œgoodbyeâ€** to exit automatically.

---

## ğŸ“ Project Structure

```
Gesture_Bridge/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ sign_to_text.py
â”œâ”€â”€ voice_to_sign.py
â”‚
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ logo.png
â”‚   â”œâ”€â”€ voicetosign.png
â”‚   â”œâ”€â”€ letters/
â”‚   â””â”€â”€ ISL_Gifs/
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ keypoint_classifier/
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ cvfpscalc.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§  Model Training (Optional)

You can retrain the gesture classifier using your own dataset:

- Capture hand landmarks using:

  - **`k`** â†’ Manual logging mode
  - **`d`** â†’ Dataset-based logging mode

- Data stored in:

  ```
  model/keypoint_classifier/keypoint.csv
  ```

- Train using the provided Jupyter Notebook.

---

## ğŸš€ Future Enhancements

- Support for **full sentence sign animation**
- Multilingual speech input
- Mobile & web-based deployment
- Enhanced dataset for more gestures
- Real-time text-to-sign avatar

---

## ğŸ‘©â€ğŸ’» Author

**Bhavya Sree**
ğŸ”— GitHub: [Bhavya0420](https://github.com/Bhavya0420)

---
